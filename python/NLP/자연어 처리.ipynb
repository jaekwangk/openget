{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cb4810",
   "metadata": {},
   "source": [
    "# 자연어 처리\n",
    "\n",
    " - 자연어는 이상용어\n",
    " - 자연어 처리는 자연어의 의미를 분석 처리하는 일\n",
    " - 텍스트 분류,감정 분석, 요약, 번역 등등등\n",
    " \n",
    " https://www.youtube.com/watch?v=2e9wnwuAVv0 참고\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf0863",
   "metadata": {},
   "source": [
    "# 텍스트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f297ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'No pain no gain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f48952f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파이썬에서 지원하는 간단한 자연어 \n",
    "'pain' in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e5948cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'pain', 'no', 'gain']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#스플릿\n",
    "s.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649520ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#스플릿\n",
    "s.split(' ').index('gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f2aa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gain'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#인덱싱 접근\n",
    "s[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e294b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gain'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split(' ')[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfb2ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()[2][::-1]#-1은 리버스 no를 on으로읽음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10d7ba",
   "metadata": {},
   "source": [
    "- 대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02f601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefgh'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#소문자 통합\n",
    "s = 'AbcdEfgH'\n",
    "s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8c5d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFGH'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#대문자 통합\n",
    "s.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2397a",
   "metadata": {},
   "source": [
    "- 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c640fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i visited UK from US on 22-09-20'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"i visited UK from US on 22-09-20\"#UK와 US 그리고 날짜는 통일시켜야함\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45bf0bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i visited United Kingdom from USB on 22-092020'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_s = s.replace(\"UK\",\"United Kingdom\").replace(\"US\",\"USB\").replace(\"-20\", \"2020\")\n",
    "new_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b523e52",
   "metadata": {},
   "source": [
    "- 문장의 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a67c48f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is a beautiful book. \n",
      " But of little use to him who caanno read it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful book. ',\n",
       " ' But of little use to him who caanno read it']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentesces = \"The world is a beautiful book. \\n But of little use to him who caanno read it\"\n",
    "print(sentesces)\n",
    "\n",
    "tokens = [x for x in sentesces.split('\\n')]#\\n을 기준으로 두개로 나눠라\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e75eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk #nltk 설치\n",
    "#import nltk#nltk 불러오기\n",
    "#nltk.download('punkt')#nltk 다운로드(이거까지 해야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c00eed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokens = sent_tokenize(sentesces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e1d7800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful book.',\n",
       " 'But of little use to him who caanno read it']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0db54fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentesces = 'Where there\\'s a will, there\\'s a way'\n",
    "tokenizer = RegexpTokenizer('[\\w]+')#모든문자(a-z,A-Z) 기준으로 최소 1글자 이상\n",
    "tokens = tokenizer.tokenize(sentesces)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b54ece66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\s]\",gaps=True)\n",
    "tokens = tokenizer.tokenize(sentesces)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f7e10",
   "metadata": {},
   "source": [
    "케라스를 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3225d3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cafd1f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from nltk>=3.1->textblob) (2022.1.18)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from click->nltk>=3.1->textblob) (4.8.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from importlib-metadata->click->nltk>=3.1->textblob) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\programdata\\anaconda3\\envs\\bigdata0809\\lib\\site-packages (from importlib-metadata->click->nltk>=3.1->textblob) (3.10.0.2)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6b7ef5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "blob = TextBlob(sentence)\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cfcbb",
   "metadata": {},
   "source": [
    "n-gram 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a929215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57efb21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'of'), ('of', 'the'), ('the', 'king')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'There is no royal of the king'\n",
    "bigram = list(ngrams(sentence.split(),2))#숫자에 따라 묶어서 만듬\n",
    "print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8b2f6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['There', 'is', 'no']),\n",
       " WordList(['is', 'no', 'royal']),\n",
       " WordList(['no', 'royal', 'of']),\n",
       " WordList(['royal', 'of', 'the']),\n",
       " WordList(['of', 'the', 'king'])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054cd9d",
   "metadata": {},
   "source": [
    "pos(품사) 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ccddd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79f5b40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thing',\n",
       " 'like',\n",
       " 'man',\n",
       " 'of',\n",
       " 'action',\n",
       " 'and',\n",
       " 'act',\n",
       " 'like',\n",
       " 'man',\n",
       " 'of',\n",
       " 'thh']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = word_tokenize(\"Thing like man of action and act like man of thh\")\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efd830ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Thing', 'VBG'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('action', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('act', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('thh', 'NN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bad7c85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('rolling', 'VBG'),\n",
       " ('stone', 'NN'),\n",
       " ('gethers', 'NNS'),\n",
       " ('no', 'DT'),\n",
       " ('moss', 'NN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"A rolling stone gethers no moss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85308818",
   "metadata": {},
   "source": [
    "불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fee4508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'in', 'the']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = \"on in the\"\n",
    "stop_words = stop_words.split(' ')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be0b36dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['singer', 'stage']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentesce = 'singer on the stage'\n",
    "sentesce = sentesce.split(' ')\n",
    "nouns = []\n",
    "for noun in sentesce:\n",
    "    if noun not in stop_words:\n",
    "        nouns.append(noun)\n",
    "nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d9729",
   "metadata": {},
   "source": [
    "nltk 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be047fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2d4c047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "166937c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'do',\n",
       " 'not',\n",
       " 'walk',\n",
       " 'today',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'have',\n",
       " 'to',\n",
       " 'run',\n",
       " 'tomorrow']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"If you do not walk today, you will have to run tomorrow\"\n",
    "words = word_tokenize(s)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0822bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stopwords = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        no_stopwords.append(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b3d5d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If', 'walk', 'today', ',', 'run', 'tomorrow']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7415c52",
   "metadata": {},
   "source": [
    "오타 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
